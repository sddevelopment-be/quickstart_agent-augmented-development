name: Build Validation (Consolidated)

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: read
  checks: write
  pull-requests: write

concurrency:
  group: build-validation-${{ github.ref }}
  cancel-in-progress: true

jobs:
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: '3.10'
          cache-key-prefix: 'pip-quality'

      - name: Install quality tools
        run: |
          python -m pip install --upgrade pip
          pip install black ruff

      - name: Check code formatting with Black
        id: black
        run: |
          echo "Checking Python code formatting..."
          if black --check src/ tests/ framework/ 2>&1 | tee black_output.txt; then
            echo "‚úÖ All Python files are properly formatted"
            echo "black_passed=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Some files need formatting"
            echo "black_passed=false" >> $GITHUB_OUTPUT
            cat black_output.txt
          fi
        continue-on-error: true

      - name: Lint with Ruff
        id: ruff
        run: |
          echo "Linting Python code..."
          if ruff check src/ tests/ framework/ 2>&1 | tee ruff_output.txt; then
            echo "‚úÖ No linting issues found"
            echo "ruff_passed=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Linting issues detected"
            echo "ruff_passed=false" >> $GITHUB_OUTPUT
            cat ruff_output.txt
          fi
        continue-on-error: true

      - name: Quality check summary
        if: always()
        run: |
          echo "# Code Quality Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.black.outputs.black_passed }}" = "true" ]; then
            echo "‚úÖ **Black formatting:** Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Black formatting:** Failed - Run \`black src/ tests/ framework/\`" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ steps.ruff.outputs.ruff_passed }}" = "true" ]; then
            echo "‚úÖ **Ruff linting:** Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Ruff linting:** Failed - Run \`ruff check src/ tests/ framework/ --fix\`" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Fail if quality checks failed
        if: steps.black.outputs.black_passed != 'true' || steps.ruff.outputs.ruff_passed != 'true'
        run: |
          echo "‚ùå Code quality checks failed"
          exit 1

  unit-tests:
    name: Unit & Integration Tests with Coverage
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: code-quality

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Setup Python environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: '3.10'
          cache-key-prefix: 'pip-tests'

      - name: Install test dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements-dev.txt ]; then
            pip install -r requirements-dev.txt
          fi
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          pip install pytest pytest-cov pytest-xdist PyYAML

      - name: Run comprehensive test suite with coverage
        id: tests
        run: |
          echo "Running comprehensive test suite with coverage..."
          python -m pytest tests/ \
            -v \
            --tb=short \
            --cov=src \
            --cov=framework \
            --cov-report=term-missing:skip-covered \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=json \
            --junit-xml=test-results.xml \
            -n auto \
            || echo "tests_passed=false" >> $GITHUB_OUTPUT
          
          if [ $? -eq 0 ]; then
            echo "‚úÖ All tests passed"
            echo "tests_passed=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Some tests failed"
            echo "tests_passed=false" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      - name: Upload coverage reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: |
            coverage.xml
            coverage.json
            htmlcov/
          retention-days: 30

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: test-results.xml
          retention-days: 30

      - name: Generate test summary
        if: always()
        run: |
          echo "# Test Coverage Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.tests.outputs.tests_passed }}" = "true" ]; then
            echo "‚úÖ **Tests:** All passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **Tests:** Some failed - Review logs for details" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Extract coverage percentage if available
          if [ -f coverage.json ]; then
            COVERAGE=$(python -c "import json; data=json.load(open('coverage.json')); print(f\"{data['totals']['percent_covered']:.1f}\")" 2>/dev/null || echo "N/A")
            echo "**Coverage:** ${COVERAGE}%" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Artifacts:** Coverage reports and test results available for download" >> $GITHUB_STEP_SUMMARY

      - name: Fail if tests failed
        if: steps.tests.outputs.tests_passed != 'true'
        run: |
          echo "‚ùå Test suite failed"
          exit 1

  validate:
    name: Validate Work Directory with Error Reporting
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: unit-tests

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      - name: Setup Python environment
        uses: ./.github/actions/setup-python-env
        with:
          python-version: '3.10'
          cache-key-prefix: 'pip-validation'
      
      # Structure Validation
      - name: Validate work directory structure
        id: structure
        run: |
          set +e  # Don't exit on error, capture output
          OUTPUT_FILE="output/structure_validation.txt"
          mkdir -p output
          
          echo "Validating work directory structure..."
          bash tools/validators/validate-work-structure.sh 2>&1 | tee "$OUTPUT_FILE"
          EXIT_CODE=${PIPESTATUS[0]}
          
          echo "exit_code=$EXIT_CODE" >> $GITHUB_OUTPUT
          echo "output_file=$OUTPUT_FILE" >> $GITHUB_OUTPUT
          
          if [ $EXIT_CODE -eq 0 ]; then
            echo "structure_valid=true" >> $GITHUB_OUTPUT
          else
            echo "structure_valid=false" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true
      
      - name: Generate structure error report
        if: always() && steps.structure.outputs.structure_valid == 'false'
        uses: ./.github/actions/error-summary
        with:
          workflow-name: "Work Directory Validation"
          job-name: "validate"
          validator-name: "work-structure"
          input-file: ${{ steps.structure.outputs.output_file }}
          fail-on-errors: 'false'
      
      # Schema Validation
      - name: Validate task YAML files
        id: schema
        run: |
          set +e
          OUTPUT_FILE="output/schema_validation.txt"
          mkdir -p output
          
          echo "Validating task YAML schemas..."
          
          # Find YAML files under done/ directories changed in the last 15 days
          task_files=$(
            git log --since="15 days ago" --name-only --pretty=format: --diff-filter=ACMR \
              ":(glob)work/**/done/**/*.yaml" \
            | sort -u \
            | while IFS= read -r file; do
                [ -f "$file" ] && [ "$file" != ".gitkeep" ] && echo "$file"
              done
          )
          
          if [ -z "$task_files" ]; then
            echo "No task files found to validate"
            echo "schema_valid=true" >> $GITHUB_OUTPUT
            echo "exit_code=0" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Validate each file and collect output
          validation_failed=0
          while IFS= read -r file; do
            echo "Validating: $file"
            if ! python tools/validators/validate-task-schema.py "$file" 2>&1 | tee -a "$OUTPUT_FILE"; then
              validation_failed=1
              echo "‚ùå Failed: $file"
            else
              echo "‚úÖ Valid: $file"
            fi
          done <<< "$task_files"
          
          echo "exit_code=$validation_failed" >> $GITHUB_OUTPUT
          echo "output_file=$OUTPUT_FILE" >> $GITHUB_OUTPUT
          
          if [ $validation_failed -eq 1 ]; then
            echo "schema_valid=false" >> $GITHUB_OUTPUT
          else
            echo "schema_valid=true" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true
      
      - name: Generate schema error report
        if: always() && steps.schema.outputs.schema_valid == 'false'
        uses: ./.github/actions/error-summary
        with:
          workflow-name: "Work Directory Validation"
          job-name: "validate"
          validator-name: "task-schema"
          input-file: ${{ steps.schema.outputs.output_file }}
          fail-on-errors: 'false'

      # Naming Convention Validation
      - name: Validate task naming conventions
        id: naming
        run: |
          set +e
          OUTPUT_FILE="output/naming_validation.txt"
          mkdir -p output
          
          echo "Validating task naming conventions..."
          bash tools/validators/validate-task-naming.sh 2>&1 | tee "$OUTPUT_FILE"
          EXIT_CODE=${PIPESTATUS[0]}
          
          echo "exit_code=$EXIT_CODE" >> $GITHUB_OUTPUT
          echo "output_file=$OUTPUT_FILE" >> $GITHUB_OUTPUT
          
          if [ $EXIT_CODE -eq 0 ]; then
            echo "naming_valid=true" >> $GITHUB_OUTPUT
          else
            echo "naming_valid=false" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      - name: Generate naming error report
        if: always() && steps.naming.outputs.naming_valid == 'false'
        uses: ./.github/actions/error-summary
        with:
          workflow-name: "Work Directory Validation"
          job-name: "validate"
          validator-name: "task-naming"
          input-file: ${{ steps.naming.outputs.output_file }}
          fail-on-errors: 'false'

      # E2E Tests
      - name: Check for E2E tests
        id: check_e2e
        run: |
          if [ -f "tests/orchestration/test_orchestration_e2e.py" ]; then
            echo "e2e_exists=true" >> $GITHUB_OUTPUT
          else
            echo "e2e_exists=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Run E2E tests
        id: e2e
        if: steps.check_e2e.outputs.e2e_exists == 'true'
        run: |
          set +e
          OUTPUT_FILE="output/e2e_validation.txt"
          mkdir -p output
          
          echo "Running E2E orchestration tests..."
          python -m pytest tests/orchestration/test_orchestration_e2e.py -v 2>&1 | tee "$OUTPUT_FILE"
          EXIT_CODE=${PIPESTATUS[0]}
          
          echo "exit_code=$EXIT_CODE" >> $GITHUB_OUTPUT
          echo "output_file=$OUTPUT_FILE" >> $GITHUB_OUTPUT
          
          if [ $EXIT_CODE -eq 0 ]; then
            echo "e2e_valid=true" >> $GITHUB_OUTPUT
          else
            echo "e2e_valid=false" >> $GITHUB_OUTPUT
          fi
        continue-on-error: true

      # Aggregate and Report
      - name: Generate unified validation summary
        id: summary
        if: always()
        run: |
          echo "# üîç Work Directory Validation Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow Run:** [${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Build summary table
          echo "| Check | Status | Details |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|--------|---------|" >> $GITHUB_STEP_SUMMARY
          
          # Structure
          if [ "${{ steps.structure.outputs.structure_valid }}" = "true" ]; then
            echo "| Work directory structure | ‚úÖ Valid | - |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Work directory structure | ‚ùå Invalid | [Error Report](./output/error-reports/) |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Schema
          if [ "${{ steps.schema.outputs.schema_valid }}" = "true" ]; then
            echo "| Task YAML schemas | ‚úÖ Valid | - |" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ steps.schema.outputs.schema_valid }}" = "false" ]; then
            echo "| Task YAML schemas | ‚ùå Invalid | [Error Report](./output/error-reports/) |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Task YAML schemas | ‚ö†Ô∏è Skipped | No files to validate |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Naming
          if [ "${{ steps.naming.outputs.naming_valid }}" = "true" ]; then
            echo "| Task naming conventions | ‚úÖ Valid | - |" >> $GITHUB_STEP_SUMMARY
          else
            echo "| Task naming conventions | ‚ùå Invalid | [Error Report](./output/error-reports/) |" >> $GITHUB_STEP_SUMMARY
          fi
          
          # E2E
          if [ "${{ steps.check_e2e.outputs.e2e_exists }}" = "true" ]; then
            if [ "${{ steps.e2e.outputs.e2e_valid }}" = "true" ]; then
              echo "| E2E tests | ‚úÖ Passed | - |" >> $GITHUB_STEP_SUMMARY
            else
              echo "| E2E tests | ‚ùå Failed | [Error Report](./output/error-reports/) |" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "| E2E tests | ‚ö†Ô∏è Skipped | Tests not available |" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Determine overall status
          all_valid=true
          [ "${{ steps.structure.outputs.structure_valid }}" != "true" ] && all_valid=false
          [ "${{ steps.schema.outputs.schema_valid }}" = "false" ] && all_valid=false
          [ "${{ steps.naming.outputs.naming_valid }}" != "true" ] && all_valid=false
          [ "${{ steps.check_e2e.outputs.e2e_exists }}" = "true" ] && [ "${{ steps.e2e.outputs.e2e_valid }}" != "true" ] && all_valid=false
          
          if [ "$all_valid" = "true" ]; then
            echo "## ‚úÖ All validations passed" >> $GITHUB_STEP_SUMMARY
            echo "validation_status=success" >> $GITHUB_OUTPUT
          else
            echo "## ‚ùå Some validations failed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ü§ñ For Copilot Agents:" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Structured error reports available:**" >> $GITHUB_STEP_SUMMARY
            echo "1. Download the \`error-summary-*\` artifacts from this run" >> $GITHUB_STEP_SUMMARY
            echo "2. Parse the JSON files for programmatic error handling" >> $GITHUB_STEP_SUMMARY
            echo "3. Review markdown files for human-readable summaries" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Quick fixes:**" >> $GITHUB_STEP_SUMMARY
            echo '```bash' >> $GITHUB_STEP_SUMMARY
            echo "# Structure issues" >> $GITHUB_STEP_SUMMARY
            echo "bash tools/validators/validate-work-structure.sh" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "# Schema errors" >> $GITHUB_STEP_SUMMARY
            echo "python tools/validators/validate-task-schema.py <file.yaml>" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "# Naming issues" >> $GITHUB_STEP_SUMMARY
            echo "bash tools/validators/validate-task-naming.sh" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "validation_status=failure" >> $GITHUB_OUTPUT
          fi

      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const validationStatus = '${{ steps.summary.outputs.validation_status }}';
            const structureValid = '${{ steps.structure.outputs.structure_valid }}' === 'true';
            const schemaValid = '${{ steps.schema.outputs.schema_valid }}' === 'true';
            const namingValid = '${{ steps.naming.outputs.naming_valid }}' === 'true';
            const e2eExists = '${{ steps.check_e2e.outputs.e2e_exists }}' === 'true';
            const e2eValid = '${{ steps.e2e.outputs.e2e_valid }}' === 'true';
            
            let body = '## üîç Work Directory Validation Results\n\n';
            body += `**Workflow Run:** [${context.runId}](${context.payload.repository.html_url}/actions/runs/${context.runId})\n\n`;
            
            body += '| Check | Status |\n';
            body += '|-------|--------|\n';
            body += `| Work directory structure | ${structureValid ? '‚úÖ Valid' : '‚ùå Invalid'} |\n`;
            body += `| Task YAML schemas | ${schemaValid ? '‚úÖ Valid' : '‚ùå Invalid'} |\n`;
            body += `| Task naming conventions | ${namingValid ? '‚úÖ Valid' : '‚ùå Invalid'} |\n`;
            body += `| E2E tests | ${e2eExists ? (e2eValid ? '‚úÖ Passed' : '‚ùå Failed') : '‚ö†Ô∏è Skipped'} |\n`;
            body += '\n';
            
            if (validationStatus === 'success') {
              body += '### ‚úÖ All validations passed!\n\n';
              body += 'Your work directory changes are valid and ready to merge.\n';
            } else {
              body += '### ‚ùå Some validations failed\n\n';
              body += '#### ü§ñ Agent-Friendly Error Reports Available\n\n';
              body += 'Download structured error reports from workflow artifacts:\n';
              body += `- \`error-summary-work-structure-${context.runId}\` (if failed)\n`;
              body += `- \`error-summary-task-schema-${context.runId}\` (if failed)\n`;
              body += `- \`error-summary-task-naming-${context.runId}\` (if failed)\n\n`;
              body += 'Each artifact contains:\n';
              body += '- **JSON**: Machine-readable error details with locations and suggestions\n';
              body += '- **Markdown**: Human-readable summary with fix instructions\n\n';
              body += '#### Quick Local Validation\n';
              body += '```bash\n';
              body += 'bash tools/validators/validate-work-structure.sh\n';
              body += 'python tools/validators/validate-task-schema.py work/path/to/task.yaml\n';
              body += 'bash tools/validators/validate-task-naming.sh\n';
              body += '```\n';
            }
            
            // Find and update/create comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('Work Directory Validation Results')
            );
            
            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }
      
      - name: Fail if validation failed
        if: steps.summary.outputs.validation_status == 'failure'
        run: |
          echo "‚ùå Validation failed - see structured error reports in artifacts"
          exit 1

  sonarqube:
    name: SonarQube
    runs-on: ubuntu-latest
    needs: unit-tests
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Shallow clones should be disabled for a better relevancy of analysis
      
      - name: Download coverage reports
        uses: actions/download-artifact@v4
        with:
          name: coverage-reports
          path: .
      
      - name: SonarQube Scan
        uses: SonarSource/sonarqube-scan-action@v6
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}