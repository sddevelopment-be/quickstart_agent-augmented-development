id: 2025-11-30T1204-build-automation-ollama-worker-pipeline
agent: build-automation
status: new
priority: normal
mode: /analysis-mode
title: Operationalize local worker nodes for Ollama tasks

artefacts:
  - ops/config/ollama_models.yaml
  - ops/scripts/run-local-worker.py
  - work/logs/build-automation/2025-11-30T1204-build-automation-ollama-worker-pipeline.md

description: |
  Implement the local worker automation described in Platform Next Steps so
  offline/private batch tasks can leverage Ollama models with structured
  logging and validation hooks.

context:
  repo: sddevelopment-be/quickstart_agent-augmented-development
  branch: main
  source_assessment: docs/architecture/assessments/platform_next_steps.md
  notes:
    - Document supported models (Llama3, DeepSeek, Codestral local builds).
    - Provide CLI flags for input prompt files, output destinations, validators.
    - Ensure privacy expectations spelled out (no external network calls).

requirements:
  - Create ops/config/ollama_models.yaml enumerating each supported local model,
    expected use cases, context windows, hardware requirements, and validation
    snippets.
  - Implement ops/scripts/run-local-worker.py able to:
    - Parse a task descriptor or prompt file.
    - Launch the relevant Ollama model.
    - Capture structured outputs plus exit codes.
    - Optionally run local validators before persisting artifacts.
  - Provide README updates or inline docstrings explaining usage and safety.

success_criteria:
  - Script runs locally (dry-run + real invocation) with sample prompts.
  - Config file validated (YAML lint) and referenced by orchestration docs.
  - Work log attached with demo results and follow-up recommendations.

created_at: '2025-11-30T12:04:00Z'
created_by: planning-petra
