# Template 5: Assessment (Evaluation & Recommendations)
# Version: 1.0.0
# Addresses: Patterns 1-3, 5-7, 9-11 from Directive 023 (Clarification Before Execution)
#
# This template is for evaluating systems, code, processes, or decisions.
# Use for code reviews, architecture assessments, or framework evaluations.

## Objective

[Clear assessment goal in 1-2 sentences. Specify what's being evaluated and why.]

Example: "Assess the current CI/CD pipeline performance and reliability, identifying bottlenecks causing >30 minute build times and recommending optimizations to achieve <10 minute builds with >95% success rate."

## Assessment Scope

**Subject:** [What is being assessed]
**Assessment Type:** [Code Review | Architecture Review | Performance Assessment | Security Audit | Process Review]
**Timeframe:** [Historical period being evaluated]

**In Scope:**
- [Area 1 to assess]
- [Area 2 to assess]
- [Area 3 to assess]

**Out of Scope:**
- [Area 1 explicitly excluded]
- [Area 2 explicitly excluded]

## Deliverables

- [ ] File: [absolute/path/to/assessment-report.md]
      Type: report
      Validation: Includes findings, metrics, recommendations with priorities

- [ ] File: [absolute/path/to/recommendations/action-plan.md] (if applicable)
      Type: doc
      Validation: Prioritized list with effort estimates

- [ ] Update: [absolute/path/to/tracking/issues.md]
      Section: [issue list]
      Change: Add high-priority findings as tracked items

**Assessment report is mandatory with quantified findings.**

## Success Criteria

- [ ] At least 5 measurable metrics collected for evaluation
- [ ] Findings categorized by severity (Critical/High/Medium/Low)
- [ ] Each finding includes evidence (not opinion)
- [ ] Recommendations are specific and actionable
- [ ] Priorities assigned based on impact and effort
- [ ] Assessment is balanced (includes positive findings)
- [ ] [Domain-specific criterion]

## Assessment Framework

### Evaluation Dimensions

Select relevant dimensions for your assessment:

#### For Code Review:
1. **Correctness** - Does it work as intended?
2. **Readability** - Is it easy to understand?
3. **Maintainability** - Can it be easily modified?
4. **Performance** - Does it meet performance requirements?
5. **Security** - Are there security vulnerabilities?
6. **Test Coverage** - Is it adequately tested?

#### For Architecture Assessment:
1. **Scalability** - Can it handle growth?
2. **Reliability** - Is it robust and fault-tolerant?
3. **Security** - Is it secure by design?
4. **Maintainability** - Can it be easily evolved?
5. **Cost Efficiency** - Is it cost-effective?
6. **Operational Complexity** - Is it operationally feasible?

#### For Performance Assessment:
1. **Response Time** - How fast does it respond?
2. **Throughput** - How much can it handle?
3. **Resource Utilization** - Is it efficient?
4. **Scalability** - Does it scale linearly?
5. **Bottlenecks** - Where are the constraints?

#### For Process Review:
1. **Efficiency** - Is it streamlined?
2. **Effectiveness** - Does it achieve goals?
3. **Compliance** - Does it follow standards?
4. **Risk Management** - Are risks handled?
5. **Continuous Improvement** - Is there feedback?

## Metrics Collection

**Required Metrics:**
Collect quantitative data to support findings:

1. [Metric 1] - Current value: [X], Target: [Y], Gap: [Z%]
2. [Metric 2] - Current value: [X], Target: [Y], Gap: [Z%]
3. [Metric 3] - Current value: [X], Target: [Y], Gap: [Z%]
4. [Metric 4] - Current value: [X], Target: [Y], Gap: [Z%]
5. [Metric 5] - Current value: [X], Target: [Y], Gap: [Z%]

**Baseline for Comparison:**
[Industry standard | Previous version | Target state | Competitor benchmark]

## Findings Structure

Use this structure for each finding:

### Finding [Number]: [Descriptive Title]

**Severity:** [Critical | High | Medium | Low]

**Category:** [Performance | Security | Maintainability | Cost | etc.]

**Description:**
[What was observed? Be specific and objective.]

**Evidence:**
[Quantitative data, metrics, examples, screenshots]
- Metric: [value]
- Example: [code/config snippet or reference]
- Impact: [quantified impact on users/system]

**Root Cause:**
[Why is this happening? What's the underlying issue?]

**Recommendation:**
[Specific action to address this finding]

**Effort Estimate:**
[Time/resources needed - e.g., "2 days, 1 developer"]

**Priority:**
[P0 (Critical - fix immediately) | P1 (High - fix this sprint) | P2 (Medium - fix next sprint) | P3 (Low - backlog)]

**Risk of Not Fixing:**
[What happens if we ignore this?]

## Constraints

### Do (Required)
- Base findings on evidence (metrics, examples, data)
- Provide actionable recommendations
- Consider implementation effort and ROI
- Include positive findings (what's working well)
- Be objective and constructive

### Don't (Prohibited)
- Don't make claims without evidence
- Don't provide vague recommendations ("improve performance")
- Don't focus only on negatives
- Don't assess areas outside stated scope
- Don't recommend solutions without considering constraints

### Time Box
[60-180 minutes recommended]

- Data collection: 30-40% of time
- Analysis: 30-40% of time
- Report writing: 20-30% of time
- Recommendations: 10-20% of time

## Context Files (Load These)

### Critical (Always Load)
1. [absolute/path/to/subject/being/assessed] - Assessment target
2. [absolute/path/to/requirements/specs] - Success criteria
3. [absolute/path/to/metrics/dashboard] - Current performance data

### Supporting (Load If Relevant)
4. [absolute/path/to/architecture/docs] - Design context
5. [absolute/path/to/previous/assessment] - Historical comparison
6. [absolute/path/to/industry/standards] - Benchmarks

### Skip (Do Not Load)
- Implementation details not relevant to assessment
- Unrelated subsystems
- Historical debates

## Compliance

- Directive 018: [.github/agents/directives/018_traceable_decisions.md](/.github/agents/directives/018_traceable_decisions.md)
- Directive 023: [.github/agents/directives/023_clarification_before_execution.md](/.github/agents/directives/023_clarification_before_execution.md)

**Mode:** /analysis-mode (required for objective assessment)

## Report Structure

### Executive Summary (Required)
- Purpose of assessment
- Key findings (3-5 most important)
- Overall health score (if applicable)
- Critical actions recommended

### Methodology (Required)
- Assessment approach
- Tools used
- Data sources
- Timeframe covered

### Detailed Findings (Required)
- All findings using structure above
- Grouped by category or severity
- Supporting evidence included

### Recommendations (Required)
- Prioritized action plan
- Effort estimates for each item
- Dependencies between recommendations
- Quick wins vs. long-term improvements

### Metrics Dashboard (Required)
- Current state metrics
- Target state metrics
- Trend analysis (if historical data available)

### Conclusion (Required)
- Overall assessment
- Risk summary
- Next steps

## Recommendations Framework

Prioritize recommendations using this matrix:

| Priority | Impact | Effort | Timing |
|----------|--------|--------|--------|
| **P0 (Critical)** | High | Any | Immediate (within 1 week) |
| **P1 (High)** | High | Low-Med | Current sprint (2-4 weeks) |
| **P2 (Medium)** | Med | Low | Next sprint (4-8 weeks) |
| **P3 (Low)** | Low-Med | Any | Backlog (when capacity available) |

**Quick Wins:**
Identify recommendations with high impact and low effort:
- [Recommendation 1] - Impact: [X], Effort: [Y hours]
- [Recommendation 2] - Impact: [X], Effort: [Y hours]

**Strategic Improvements:**
Long-term improvements requiring more effort:
- [Recommendation 1] - Impact: [X], Effort: [Y weeks]
- [Recommendation 2] - Impact: [X], Effort: [Y weeks]

## Checkpoints (For assessments >90 minutes)

- [ ] Checkpoint 1 (30 min): Data collection complete, metrics gathered
- [ ] Checkpoint 2 (60 min): Analysis complete, findings identified
- [ ] Checkpoint 3 (90 min): Recommendations drafted with priorities
- [ ] Checkpoint 4 (120 min): Report written and reviewed

## Quality Assurance

Before finalizing assessment:

- [ ] All findings have supporting evidence
- [ ] Metrics are accurate and up-to-date
- [ ] Recommendations are specific and actionable
- [ ] Priorities align with business/technical constraints
- [ ] Assessment is balanced (positive + negative)
- [ ] Report is readable by intended audience
- [ ] Executive summary captures key points

## Handoff (If Required)

For implementing recommendations:

**Next Agent:** [architect | build-automation | specific-agent]
**Next Task Title:** "Implement P0/P1 recommendations from [assessment name]"
**Context to Carry Forward:**
- Assessment findings: [summary]
- Prioritized recommendations: [top 3-5]
- Constraints: [technical/business limitations]
- Success metrics: [how to measure improvement]

## Token Budget

- **Target Input:** 22,000 tokens (subject + context + standards)
- **Estimated Output:** 3,000-6,000 tokens (comprehensive report)

## Comparison Baseline

If applicable, compare against:

**Baseline Type:** [Previous version | Industry standard | Target state]

**Comparison Metrics:**
| Metric | Baseline | Current | Change | Target |
|--------|----------|---------|--------|--------|
| [Metric 1] | [X] | [Y] | [+/-Z%] | [Target] |
| [Metric 2] | [X] | [Y] | [+/-Z%] | [Target] |
| [Metric 3] | [X] | [Y] | [+/-Z%] | [Target] |

## Notes

- **Assessment Triggers:** [What prompted this assessment]
- **Known Limitations:** [Scope limitations or data gaps]
- **Follow-up Schedule:** [When to reassess]
- **Stakeholder Distribution:** [Who needs this report]

---

**Template Version:** 1.0.0  
**Last Updated:** 2026-01-30  
**Related:** Directive 023 (Clarification Before Execution) Prompt Optimization Framework
